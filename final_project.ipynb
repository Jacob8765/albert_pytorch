{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Final Project\n",
    "\n",
    "Students: Jose Pujol, Jacob Schuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load and Inspection of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(os.getcwd(), 'dataset', 'Stock News Dataset.csv'), encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {data.shape[0]}\")\n",
    "print(f\"Number of columns: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"From {data['Date'].min()} to {data['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains information on the top 25 headlines for a stock. The first two columns are the date and the label. The label is a 1 if the DJIA adjusted close value rose or stayed the same, and a 0 if it decreased. The remaining columns are the top 25 headlines for that day. The data is from January 2003 to July 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the date is a string, the label is an integer 0 or 1. And the rest of the columns are strings which are the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have 3 rows that have null values. We will drop these rows since it is a small amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date column is mostly irrelavent and could cause the model to overfit, so let's drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {data.shape[0]}\")\n",
    "print(f\"Number of columns: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = data['Label'].unique()\n",
    "\n",
    "for class_name in all_classes:\n",
    "    print(f\"Percentage of {class_name} class in dataset: {data[data['Label'] == class_name]['Label'].size/data['Label'].size*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a wide disparity between the class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fact that we are dealing with sequential data here, you cannot just randomly split the data into training and testing sets. We will split the data into training and testing sets based on the date. We will use the first 72% of the data for training and the last 28% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feauture Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_model import inference_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the textual article headlines to a sentiment score (0-1) using a transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inference_model(data)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe just in case\n",
    "data.to_csv(os.path.join(os.getcwd(), 'dataset', 'Stock News Dataset (with sentiment).csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = data['Label']\n",
    "X = data.drop(['Label'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows in train dataset: {X_train.shape[0]}, {X_train.shape[0]/data.shape[0]*100:0.2f}%\")\n",
    "print(f\"Number of rows in test dataset: {X_test.shape[0]}, {X_test.shape[0]/data.shape[0]*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame(columns=[\n",
    "                                \"Model Name\",\n",
    "                                \"Notes\",\n",
    "                                \"Accuracy (Training)\",\n",
    "                                \"Log Loss (Training)\",\n",
    "                                \"F1 Score (Training)\",\n",
    "                                \"Precision (Training)\",\n",
    "                                \"Recall (Training)\",\n",
    "                                \"Accuracy (Validation)\",\n",
    "                                \"Log Loss (Validation)\",\n",
    "                                \"F1 Score (Validation)\",\n",
    "                                \"Precision (Validation)\",\n",
    "                                \"Recall (Validation)\",])\n",
    "models_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model_stats(model, model_name, notes, X_train, y_train, X_val, y_val, models_df):\n",
    "    X_train_predictions = model.predict(X_train)\n",
    "    X_train_accuracy = accuracy_score(y_train, X_train_predictions)\n",
    "    print(f\"{model_name} validation Accuracy Score on training set: {X_train_accuracy:0.4f}\")\n",
    "\n",
    "    X_val_predictions = model.predict(X_val)\n",
    "    X_val_accuracy = accuracy_score(y_val, X_val_predictions)\n",
    "    print(f\"{model_name} validation Accuracy Score on validation set: {X_val_accuracy:0.4f}\")\n",
    "\n",
    "    # Get probabilities\n",
    "    X_train_probabilities = model.predict_proba(X_train)\n",
    "    X_val_probabilities = model.predict_proba(X_val)\n",
    "\n",
    "    log_loss_training_set = log_loss(y_train, X_train_probabilities)\n",
    "    print(f\"{model_name} validation Log Loss on validation set: {log_loss_training_set:0.4f}\")\n",
    "\n",
    "    log_loss_validation_set = log_loss(y_val, X_val_probabilities)\n",
    "    print(f\"{model_name} validation Log Loss on validation set: {log_loss_validation_set:0.4f}\")\n",
    "\n",
    "    model_f1_train = f1_score(y_train, X_train_predictions, average='weighted')\n",
    "    model_precision_train = precision_score(y_train, X_train_predictions, average=\"weighted\")\n",
    "    model_recall_train = recall_score(y_train, X_train_predictions, average=\"weighted\")\n",
    "\n",
    "    model_f1_val = f1_score(y_val, X_val_predictions, average='weighted')\n",
    "    print(f\"{model_name} validation F1 Score on validation set: {model_f1_val:0.4f}\")\n",
    "    model_precision_val = precision_score(y_val, X_val_predictions, average=\"weighted\")\n",
    "    print(f\"{model_name} validation Precision Score on validation set: {model_precision_val:0.4f}\")\n",
    "    model_recall_val = recall_score(y_val, X_val_predictions, average=\"weighted\")\n",
    "    print(f\"{model_name} validation Recall Score on validation set: {model_recall_val:0.4f}\")\n",
    "\n",
    "    df_entry = {\n",
    "            \"Model Name\": model_name,\n",
    "            \"Notes\": notes,\n",
    "            \"Accuracy (Training)\": X_train_accuracy,\n",
    "            \"Log Loss (Training)\": log_loss_training_set,\n",
    "            \"F1 Score (Training)\": model_f1_train,\n",
    "            \"Precision (Training)\": model_precision_train,\n",
    "            \"Recall (Training)\": model_recall_train,\n",
    "            \"Accuracy (Validation)\": X_val_accuracy,\n",
    "            \"Log Loss (Validation)\": log_loss_validation_set,\n",
    "            \"F1 Score (Validation)\": model_f1_val,\n",
    "            \"Precision (Validation)\": model_precision_val,\n",
    "            \"Recall (Validation)\": model_recall_val}\n",
    "\n",
    "    # As of Pandas 2.0 append is deprecated and we have to use concat now, not a fan\n",
    "    # See this post https://stackoverflow.com/questions/75956209/error-dataframe-object-has-no-attribute-append\n",
    "    models_df = pd.concat([models_df, pd.DataFrame([df_entry])], ignore_index=True)\n",
    "    return models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(random_state=42, max_iter=10000)\n",
    "svc_model_linear = SVC(random_state=42, kernel=\"linear\", probability=True, gamma=\"auto\")\n",
    "svc_model_rbf = SVC(random_state=42, kernel=\"rbf\", probability=True, gamma=\"auto\")\n",
    "random_forest_classifier = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=200, criterion=\"entropy\")\n",
    "xgb_classifier = XGBClassifier(random_state=42, n_jobs=-1, n_estimators=200, gamma=0.1, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logistic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix=confusion_matrix(y_test, predictions)\n",
    "print(matrix)\n",
    "score=accuracy_score(y_test, predictions)\n",
    "print(score)\n",
    "report=classification_report(y_test, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model_linear.fit(train, train['Label'])\n",
    "predictions = svc_model_linear.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix=confusion_matrix(test['Label'],predictions)\n",
    "print(matrix)\n",
    "score=accuracy_score(test['Label'],predictions)\n",
    "print(score)\n",
    "report=classification_report(test['Label'],predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model_rbf.fit(train, train['Label'])\n",
    "predictions = svc_model_rbf.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix=confusion_matrix(test['Label'],predictions)\n",
    "print(matrix)\n",
    "score=accuracy_score(test['Label'],predictions)\n",
    "print(score)\n",
    "report=classification_report(test['Label'],predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier.fit(X_train, y_train)\n",
    "predictions = random_forest_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix=confusion_matrix(y_test, predictions)\n",
    "print(matrix)\n",
    "score=accuracy_score(y_test, predictions)\n",
    "print(score)\n",
    "report=classification_report(y_test, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier.fit(train, train['Label'])\n",
    "predictions = xgb_classifier.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix=confusion_matrix(test['Label'],predictions)\n",
    "print(matrix)\n",
    "score=accuracy_score(test['Label'],predictions)\n",
    "print(score)\n",
    "report=classification_report(test['Label'],predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Insights and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
